{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JxrcJu64gGdz"
   },
   "outputs": [],
   "source": [
    "#uses pandas and then convert to list/dict\n",
    "import pandas as pd \n",
    "import string\n",
    "from string import punctuation\n",
    "import re\n",
    "import emoji\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.utils import shuffle\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import itertools\n",
    "import time\n",
    "import contractions\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn.metrics as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwBf8mpmhML7"
   },
   "outputs": [],
   "source": [
    "data_folder = 'data/'\n",
    "#glove_folder = 'E:/rajiur/workspace/nlp/data/'\n",
    "glove_folder = 'D:/workspace/nlp/glove/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = pd.read_pickle('train_tokenized_df.pkl')\n",
    "dfdev = pd.read_pickle('dev_tokenized_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set = dfdev.to_dict('records')\n",
    "train_set = dftrain.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.pkl', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    vocabulary = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22138"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Embedder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YodY1Tg6jPZP"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class WordEmbedder(nn.Module):\n",
    "    def __init__(self, vocab, glove_file, seqlen = 300, varlen = False):\n",
    "        super(WordEmbedder, self).__init__()\n",
    "        assert os.path.exists(glove_file) and glove_file.endswith('.txt'), glove_file\n",
    "        \n",
    "        self.emb_dim = None\n",
    "        \n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.UNK_TOKEN = '<UNK>'\n",
    "        self.sequence_length = seqlen\n",
    "        self.various_length = varlen\n",
    "        \n",
    "        idx2word = [self.PAD_TOKEN, self.UNK_TOKEN]\n",
    "        idx2vect = [None, None]\n",
    "        \n",
    "        with open(glove_file, 'r', encoding='utf-8') as fp:\n",
    "            for line in fp:\n",
    "                line = line.split()\n",
    "                \n",
    "                if line[0] not in vocab:\n",
    "                    continue\n",
    "                \n",
    "                w = line[0]\n",
    "                v = np.array([float(value) for value in line[1:]])\n",
    "                \n",
    "                if self.emb_dim is None:\n",
    "                    self.emb_dim = v.shape[0]\n",
    "            \n",
    "                idx2word.append(w)\n",
    "                idx2vect.append(v)\n",
    "                \n",
    "        idx2vect[0] = np.zeros(self.emb_dim)\n",
    "        idx2vect[1] = np.mean(idx2vect[2:], axis=0)\n",
    "    \n",
    "        self.embeddings = torch.from_numpy(np.array(idx2vect)).float()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(self.embeddings, freeze=False)\n",
    "        \n",
    "        self.idx2word = {i: w for i, w in enumerate(idx2word)}\n",
    "        self.word2idx = {w: i for i, w in self.idx2word.items()}\n",
    "    \n",
    "    def forward(self, samples):\n",
    "        pad_idx = self.word2idx[self.PAD_TOKEN]\n",
    "        unk_idx = self.word2idx[self.UNK_TOKEN]\n",
    "        \n",
    "        if self.various_length:\n",
    "\n",
    "            #Find the length of the longest sample\n",
    "            maxlen = max([len(s) for s in samples])\n",
    "        \n",
    "        else:\n",
    "\n",
    "            #Use a constant length for all samples\n",
    "            maxlen = self.sequence_length\n",
    "        \n",
    "        encoded = [[self.word2idx.get(token, unk_idx) for token in tokens] for tokens in samples]\n",
    "        \n",
    "        padded = np.zeros((len(samples), maxlen), dtype=int)\n",
    "        masks = torch.zeros(len(samples), maxlen).long()\n",
    "        \n",
    "        # Padding and masking\n",
    "        for i in range(len(encoded)):\n",
    "            masks[i, :len(encoded[i])] = 1\n",
    "            padded[i, :len(encoded[i])] = np.array(encoded[i])[:maxlen]\n",
    "            # encoded[i] += [pad_idx] * max(0, (maxlen - len(encoded[i])))\n",
    "        \n",
    "        encoded = torch.tensor(padded).long()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            encoded = encoded.cuda()\n",
    "            masks = masks.cuda()\n",
    "        \n",
    "        result = {\n",
    "            'output': self.embeddings(encoded),\n",
    "            'mask': masks,\n",
    "            'encoded': encoded\n",
    "        }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "pEp3vP0ljUk8",
    "outputId": "0910836d-5434-4610-8c5e-f526d6587664"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordEmbedder(\n",
       "  (embeddings): Embedding(19447, 300)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = torch.load('embedder_glove_300d.pt')\n",
    "embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_-JCBPkkoNq"
   },
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a2bFiexQks5Y"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def track_best_model(model_path, model, epoch, best_acc, dev_acc, dev_loss):\n",
    "    if best_acc > dev_acc:\n",
    "        return best_acc, ''\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'acc': dev_acc,\n",
    "        'loss': dev_loss,\n",
    "        'model': model.state_dict()\n",
    "    }\n",
    "    torch.save(state, model_path)\n",
    "    return dev_acc, ' * '\n",
    "\n",
    "def hot_target(batch_target):\n",
    "    result = []\n",
    "    for target in batch_target:\n",
    "        init_target = [0,0,0]\n",
    "        init_target[target]=1\n",
    "        result.append(init_target)\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XQxNjqZEktf_"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def train(model, optimizer, shuffled_train_set, batch_size, loss_func=\"CE\"):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss().to(\"cpu\")\n",
    "    total_loss = 0\n",
    "    batch_tokens, batch_target = [], []\n",
    "\n",
    "    random.Random(1234).shuffle(shuffled_train_set)\n",
    "\n",
    "    for i in range(len(shuffled_train_set)):\n",
    "\n",
    "        batch_tokens.append(shuffled_train_set[i]['tokens'])\n",
    "        batch_target.append(shuffled_train_set[i]['labelnumber'])\n",
    "\n",
    "        if len(batch_tokens) == batch_size or i == (len(shuffled_train_set) - 1):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(batch_tokens)\n",
    "            y_pred = out.cpu()\n",
    "\n",
    "            if(loss_func=='CE'):\n",
    "                #loss method 1: CrossEntropyLoss\n",
    "                loss = criterion(y_pred, torch.tensor(batch_target))\n",
    "            else:\n",
    "                loss = torch.nn.functional.binary_cross_entropy(y_pred,torch.tensor(hot_target(batch_target)).float())\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_tokens, batch_target = [], []\n",
    "        \n",
    "    return model, shuffled_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5sZMJ9fku9k"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dev_set, batch_size, loss_func='CE'):\n",
    "\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(\"cpu\")\n",
    "    total_loss = 0\n",
    "    accurate = 0\n",
    "    batch_tokens, batch_target = [], []\n",
    "    predictions, actual = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dev_set)):\n",
    "            \n",
    "            batch_tokens.append(dev_set[i]['tokens'])\n",
    "            batch_target.append(dev_set[i]['labelnumber'])\n",
    "\n",
    "            if len(batch_tokens) == batch_size or i == (len(dev_set)-1):\n",
    "\n",
    "                out = model(batch_tokens)\n",
    "                #print(\"out\",out)\n",
    "                y_pred = out.cpu()\n",
    "                \n",
    "                predictions.extend(y_pred.argmax(1).tolist())\n",
    "                actual.extend(batch_target)\n",
    "               \n",
    "                if(loss_func=='CE'):\n",
    "                    loss = criterion(y_pred, torch.tensor(batch_target))\n",
    "                else:\n",
    "                    loss = torch.nn.functional.binary_cross_entropy(y_pred,torch.tensor(hot_target(batch_target)).float())\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                accurate += (y_pred.argmax(1) == torch.tensor(batch_target)).sum().item()\n",
    "                \n",
    "                 \n",
    "\n",
    "                batch_tokens, batch_target = [], []\n",
    "\n",
    "    return predictions, total_loss/len(dev_set), accurate/len(dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ko3ufksekwnL"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def training_loop(config, model_, train_set, dev_set, loss_func='CE'):\n",
    "\n",
    "    model = model_\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=config['momentum'])\n",
    "\n",
    "    shuffled_train_set = train_set\n",
    "    best_acc = 0\n",
    "    \n",
    "\n",
    "    for epoch in range(config['epoch']):\n",
    "\n",
    "        epoch_msg = '[Epoch {}] / {}'.format(epoch+1, config['epoch'])\n",
    "\n",
    "        model, shuffled_train_set = train(model, optimizer, shuffled_train_set, batch_size=64, loss_func=loss_func)\n",
    "\n",
    "        train_pred, train_loss, train_acc = evaluate(model, shuffled_train_set, batch_size=128, loss_func=loss_func)\n",
    "        epoch_msg += ' [TRAIN] Loss: {:.4f}, Acc: {:.4f}'.format(train_loss, train_acc)\n",
    "        val_pred, val_loss, val_acc = evaluate(model, dev_set, batch_size=128, loss_func=loss_func)\n",
    "        epoch_msg += ' [DEV] Loss: {:.4f}, Acc: {:.4f}'.format(val_loss, val_acc)\n",
    "        #test_pred, test_loss, test_acc = evaluate(model, test_set, batch_size=128)\n",
    "        #epoch_msg += ' [TEST] Loss: {:.4f}, Acc: {:.4f}'.format(test_loss, test_acc)\n",
    "\n",
    "        best_acc, epoch_track = track_best_model(config['checkpoint'], model, epoch, best_acc, val_acc, val_loss)\n",
    "        print(epoch_msg + epoch_track)\n",
    "\n",
    "    print('Done Training!')\n",
    "\n",
    "    state = torch.load(config['checkpoint'])\n",
    "    model.load_state_dict(state['model'])\n",
    "\n",
    "    print('Returning best model from epoch {} with loss {:.5f} and accuracy {:.5f}'.format(state['epoch'], state['loss'], state['acc']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(RNNLayer, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, bidirectional=False, dropout = 0.1, num_layers = 2)\n",
    "\n",
    "    def forward(self, vectors, mask):\n",
    "        batch_size = vectors.size(0)\n",
    "        maxlen = vectors.size(1)\n",
    "        lengths = mask.sum(-1)\n",
    "        \n",
    "        rnn_out, _ = self.rnn(vectors)  # (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        assert rnn_out.size(0) == batch_size\n",
    "        assert rnn_out.size(1) == maxlen\n",
    "        assert rnn_out.size(2) == self.hidden_dim\n",
    "\n",
    "        last_hn = rnn_out[range(batch_size), lengths-1]        # (batch, hidden)\n",
    "\n",
    "        return {'output': last_hn, 'outputs': rnn_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLayer(\n",
       "  (rnn): RNN(300, 64, num_layers=2, dropout=0.1)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_layer = RNNLayer(embedder.emb_dim, 64)\n",
    "rnn_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vWAgU01GkXmC"
   },
   "outputs": [],
   "source": [
    "class MyClassifier(nn.Module):\n",
    "    def __init__(self, embedder, extractor):\n",
    "        super(MyClassifier, self).__init__()\n",
    "        self.embedder = embedder\n",
    "        self.extractor = extractor\n",
    "        self.classifier = nn.Linear(extractor.hidden_dim, 3)\n",
    "        #self.fc1 = nn.Linear(extractor.hidden_dim, 128)\n",
    "        #self.fc2 = nn.Linear(128, 64)\n",
    "        #self.classifier = nn.Linear(64, 3)\n",
    "        \n",
    "    def forward(self, tokens, targets=None):\n",
    "        embedded = self.embedder(tokens)\n",
    "        extracted = self.extractor(embedded['output'], embedded['mask'])\n",
    "        \n",
    "        #print(type(self.classifier(extracted['output'])),self.classifier(extracted['output']).shape,self.classifier(extracted['output']))\n",
    "        #print(type(torch.sigmoid(self.classifier(extracted['output']))), torch.sigmoid(self.classifier(extracted['output'])).shape, torch.sigmoid(self.classifier(extracted['output'])))\n",
    "        #print(torch.softmax(self.classifier(extracted['output']), dim=1))\n",
    "        \n",
    "        #x = self.fc1(extracted['output'])\n",
    "        #x = self.fc2(x)\n",
    "        #output = torch.softmax(self.classifier(x),dim=1)\n",
    "        output = torch.softmax(self.classifier(extracted['output']), dim=1)\n",
    "       \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "Cic38lQQkf-I",
    "outputId": "ed3f5d54-ecf0-46a7-e5ec-82ca82cc53d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyClassifier(\n",
       "  (embedder): WordEmbedder(\n",
       "    (embeddings): Embedding(19447, 300)\n",
       "  )\n",
       "  (extractor): RNNLayer(\n",
       "    (rnn): RNN(300, 64, num_layers=2, dropout=0.1)\n",
       "  )\n",
       "  (classifier): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model = MyClassifier(embedder, rnn_layer)\n",
    "rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FKcCnm5rkis7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] / 100 [TRAIN] Loss: 0.0080, Acc: 0.4911 [DEV] Loss: 0.0083, Acc: 0.4650 * \n",
      "[Epoch 2] / 100 [TRAIN] Loss: 0.0080, Acc: 0.4880 [DEV] Loss: 0.0084, Acc: 0.4615\n",
      "[Epoch 3] / 100 [TRAIN] Loss: 0.0080, Acc: 0.4982 [DEV] Loss: 0.0085, Acc: 0.4585\n",
      "[Epoch 4] / 100 [TRAIN] Loss: 0.0078, Acc: 0.5235 [DEV] Loss: 0.0084, Acc: 0.4495\n",
      "[Epoch 5] / 100 [TRAIN] Loss: 0.0078, Acc: 0.5381 [DEV] Loss: 0.0084, Acc: 0.4615\n",
      "[Epoch 6] / 100 [TRAIN] Loss: 0.0079, Acc: 0.5220 [DEV] Loss: 0.0086, Acc: 0.4400\n",
      "[Epoch 7] / 100 [TRAIN] Loss: 0.0077, Acc: 0.5515 [DEV] Loss: 0.0086, Acc: 0.4525\n",
      "[Epoch 8] / 100 [TRAIN] Loss: 0.0076, Acc: 0.5737 [DEV] Loss: 0.0085, Acc: 0.4635\n",
      "[Epoch 9] / 100 [TRAIN] Loss: 0.0075, Acc: 0.5854 [DEV] Loss: 0.0085, Acc: 0.4630\n",
      "[Epoch 10] / 100 [TRAIN] Loss: 0.0080, Acc: 0.5123 [DEV] Loss: 0.0087, Acc: 0.4530\n",
      "[Epoch 11] / 100 [TRAIN] Loss: 0.0075, Acc: 0.5881 [DEV] Loss: 0.0085, Acc: 0.4655 * \n",
      "[Epoch 12] / 100 [TRAIN] Loss: 0.0075, Acc: 0.5873 [DEV] Loss: 0.0087, Acc: 0.4415\n",
      "[Epoch 13] / 100 [TRAIN] Loss: 0.0074, Acc: 0.5969 [DEV] Loss: 0.0086, Acc: 0.4585\n",
      "[Epoch 14] / 100 [TRAIN] Loss: 0.0074, Acc: 0.5963 [DEV] Loss: 0.0086, Acc: 0.4600\n",
      "[Epoch 15] / 100 [TRAIN] Loss: 0.0073, Acc: 0.6177 [DEV] Loss: 0.0086, Acc: 0.4590\n",
      "[Epoch 16] / 100 [TRAIN] Loss: 0.0073, Acc: 0.6130 [DEV] Loss: 0.0086, Acc: 0.4590\n",
      "[Epoch 17] / 100 [TRAIN] Loss: 0.0074, Acc: 0.5926 [DEV] Loss: 0.0085, Acc: 0.4615\n",
      "[Epoch 18] / 100 [TRAIN] Loss: 0.0072, Acc: 0.6226 [DEV] Loss: 0.0086, Acc: 0.4640\n",
      "[Epoch 19] / 100 [TRAIN] Loss: 0.0073, Acc: 0.6168 [DEV] Loss: 0.0086, Acc: 0.4640\n",
      "[Epoch 20] / 100 [TRAIN] Loss: 0.0073, Acc: 0.6134 [DEV] Loss: 0.0087, Acc: 0.4500\n",
      "[Epoch 21] / 100 [TRAIN] Loss: 0.0072, Acc: 0.6241 [DEV] Loss: 0.0087, Acc: 0.4540\n",
      "[Epoch 22] / 100 [TRAIN] Loss: 0.0072, Acc: 0.6248 [DEV] Loss: 0.0086, Acc: 0.4565\n",
      "[Epoch 23] / 100 [TRAIN] Loss: 0.0073, Acc: 0.6197 [DEV] Loss: 0.0087, Acc: 0.4575\n",
      "[Epoch 24] / 100 [TRAIN] Loss: 0.0072, Acc: 0.6209 [DEV] Loss: 0.0087, Acc: 0.4490\n",
      "[Epoch 25] / 100 [TRAIN] Loss: 0.0072, Acc: 0.6318 [DEV] Loss: 0.0087, Acc: 0.4540\n",
      "[Epoch 26] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6337 [DEV] Loss: 0.0088, Acc: 0.4410\n",
      "[Epoch 27] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6357 [DEV] Loss: 0.0086, Acc: 0.4580\n",
      "[Epoch 28] / 100 [TRAIN] Loss: 0.0072, Acc: 0.6207 [DEV] Loss: 0.0087, Acc: 0.4510\n",
      "[Epoch 29] / 100 [TRAIN] Loss: 0.0072, Acc: 0.6212 [DEV] Loss: 0.0087, Acc: 0.4550\n",
      "[Epoch 30] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6352 [DEV] Loss: 0.0087, Acc: 0.4530\n",
      "[Epoch 31] / 100 [TRAIN] Loss: 0.0072, Acc: 0.6330 [DEV] Loss: 0.0087, Acc: 0.4495\n",
      "[Epoch 32] / 100 [TRAIN] Loss: 0.0072, Acc: 0.6332 [DEV] Loss: 0.0086, Acc: 0.4625\n",
      "[Epoch 33] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6358 [DEV] Loss: 0.0087, Acc: 0.4585\n",
      "[Epoch 34] / 100 [TRAIN] Loss: 0.0072, Acc: 0.6352 [DEV] Loss: 0.0088, Acc: 0.4420\n",
      "[Epoch 35] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6368 [DEV] Loss: 0.0087, Acc: 0.4465\n",
      "[Epoch 36] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6351 [DEV] Loss: 0.0086, Acc: 0.4555\n",
      "[Epoch 37] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6389 [DEV] Loss: 0.0087, Acc: 0.4525\n",
      "[Epoch 38] / 100 [TRAIN] Loss: 0.0072, Acc: 0.6342 [DEV] Loss: 0.0087, Acc: 0.4540\n",
      "[Epoch 39] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6339 [DEV] Loss: 0.0087, Acc: 0.4535\n",
      "[Epoch 40] / 100 [TRAIN] Loss: 0.0072, Acc: 0.6344 [DEV] Loss: 0.0087, Acc: 0.4520\n",
      "[Epoch 41] / 100 [TRAIN] Loss: 0.0072, Acc: 0.6347 [DEV] Loss: 0.0087, Acc: 0.4530\n",
      "[Epoch 42] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6362 [DEV] Loss: 0.0086, Acc: 0.4645\n",
      "[Epoch 43] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6409 [DEV] Loss: 0.0087, Acc: 0.4505\n",
      "[Epoch 44] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6381 [DEV] Loss: 0.0087, Acc: 0.4530\n",
      "[Epoch 45] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6424 [DEV] Loss: 0.0087, Acc: 0.4490\n",
      "[Epoch 46] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6395 [DEV] Loss: 0.0087, Acc: 0.4555\n",
      "[Epoch 47] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6440 [DEV] Loss: 0.0087, Acc: 0.4500\n",
      "[Epoch 48] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6453 [DEV] Loss: 0.0087, Acc: 0.4590\n",
      "[Epoch 49] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6428 [DEV] Loss: 0.0087, Acc: 0.4535\n",
      "[Epoch 50] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6398 [DEV] Loss: 0.0087, Acc: 0.4540\n",
      "[Epoch 51] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6399 [DEV] Loss: 0.0087, Acc: 0.4545\n",
      "[Epoch 52] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6397 [DEV] Loss: 0.0087, Acc: 0.4630\n",
      "[Epoch 53] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6466 [DEV] Loss: 0.0087, Acc: 0.4605\n",
      "[Epoch 54] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6332 [DEV] Loss: 0.0087, Acc: 0.4565\n",
      "[Epoch 55] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6480 [DEV] Loss: 0.0087, Acc: 0.4590\n",
      "[Epoch 56] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6398 [DEV] Loss: 0.0087, Acc: 0.4505\n",
      "[Epoch 57] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6461 [DEV] Loss: 0.0087, Acc: 0.4585\n",
      "[Epoch 58] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6372 [DEV] Loss: 0.0087, Acc: 0.4620\n",
      "[Epoch 59] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6399 [DEV] Loss: 0.0087, Acc: 0.4640\n",
      "[Epoch 60] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6422 [DEV] Loss: 0.0087, Acc: 0.4565\n",
      "[Epoch 61] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6494 [DEV] Loss: 0.0087, Acc: 0.4505\n",
      "[Epoch 62] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6476 [DEV] Loss: 0.0087, Acc: 0.4625\n",
      "[Epoch 63] / 100 [TRAIN] Loss: 0.0070, Acc: 0.6486 [DEV] Loss: 0.0086, Acc: 0.4625\n",
      "[Epoch 64] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6384 [DEV] Loss: 0.0087, Acc: 0.4620\n",
      "[Epoch 65] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6430 [DEV] Loss: 0.0086, Acc: 0.4675 * \n",
      "[Epoch 66] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6428 [DEV] Loss: 0.0087, Acc: 0.4590\n",
      "[Epoch 67] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6478 [DEV] Loss: 0.0087, Acc: 0.4520\n",
      "[Epoch 68] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6488 [DEV] Loss: 0.0087, Acc: 0.4545\n",
      "[Epoch 69] / 100 [TRAIN] Loss: 0.0070, Acc: 0.6489 [DEV] Loss: 0.0087, Acc: 0.4600\n",
      "[Epoch 70] / 100 [TRAIN] Loss: 0.0070, Acc: 0.6510 [DEV] Loss: 0.0086, Acc: 0.4615\n",
      "[Epoch 71] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6476 [DEV] Loss: 0.0087, Acc: 0.4590\n",
      "[Epoch 72] / 100 [TRAIN] Loss: 0.0070, Acc: 0.6491 [DEV] Loss: 0.0087, Acc: 0.4570\n",
      "[Epoch 73] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6465 [DEV] Loss: 0.0087, Acc: 0.4570\n",
      "[Epoch 74] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6475 [DEV] Loss: 0.0087, Acc: 0.4625\n",
      "[Epoch 75] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6456 [DEV] Loss: 0.0087, Acc: 0.4545\n",
      "[Epoch 76] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6415 [DEV] Loss: 0.0087, Acc: 0.4610\n",
      "[Epoch 77] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6452 [DEV] Loss: 0.0087, Acc: 0.4515\n",
      "[Epoch 78] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6489 [DEV] Loss: 0.0087, Acc: 0.4545\n",
      "[Epoch 79] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6485 [DEV] Loss: 0.0088, Acc: 0.4455\n",
      "[Epoch 80] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6489 [DEV] Loss: 0.0087, Acc: 0.4595\n",
      "[Epoch 81] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6421 [DEV] Loss: 0.0087, Acc: 0.4550\n",
      "[Epoch 82] / 100 [TRAIN] Loss: 0.0072, Acc: 0.6360 [DEV] Loss: 0.0087, Acc: 0.4570\n",
      "[Epoch 83] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6489 [DEV] Loss: 0.0087, Acc: 0.4495\n",
      "[Epoch 84] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6462 [DEV] Loss: 0.0088, Acc: 0.4505\n",
      "[Epoch 85] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6452 [DEV] Loss: 0.0088, Acc: 0.4395\n",
      "[Epoch 86] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6432 [DEV] Loss: 0.0088, Acc: 0.4390\n",
      "[Epoch 87] / 100 [TRAIN] Loss: 0.0070, Acc: 0.6498 [DEV] Loss: 0.0087, Acc: 0.4545\n",
      "[Epoch 88] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6488 [DEV] Loss: 0.0087, Acc: 0.4525\n",
      "[Epoch 89] / 100 [TRAIN] Loss: 0.0070, Acc: 0.6501 [DEV] Loss: 0.0088, Acc: 0.4500\n",
      "[Epoch 90] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6476 [DEV] Loss: 0.0087, Acc: 0.4540\n",
      "[Epoch 91] / 100 [TRAIN] Loss: 0.0070, Acc: 0.6492 [DEV] Loss: 0.0087, Acc: 0.4590\n",
      "[Epoch 92] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6429 [DEV] Loss: 0.0087, Acc: 0.4575\n",
      "[Epoch 93] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6465 [DEV] Loss: 0.0087, Acc: 0.4565\n",
      "[Epoch 94] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6438 [DEV] Loss: 0.0087, Acc: 0.4565\n",
      "[Epoch 95] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6492 [DEV] Loss: 0.0087, Acc: 0.4625\n",
      "[Epoch 96] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6462 [DEV] Loss: 0.0087, Acc: 0.4500\n",
      "[Epoch 97] / 100 [TRAIN] Loss: 0.0070, Acc: 0.6512 [DEV] Loss: 0.0087, Acc: 0.4540\n",
      "[Epoch 98] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6429 [DEV] Loss: 0.0087, Acc: 0.4545\n",
      "[Epoch 99] / 100 [TRAIN] Loss: 0.0070, Acc: 0.6493 [DEV] Loss: 0.0087, Acc: 0.4540\n",
      "[Epoch 100] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6468 [DEV] Loss: 0.0087, Acc: 0.4530\n",
      "Done Training!\n",
      "Returning best model from epoch 64 with loss 0.00862 and accuracy 0.46750\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'lr': 0.2,#1e-3,\n",
    "    'momentum': 0.9, #0.99,\n",
    "    'epoch': 100,\n",
    "    'checkpoint': 'rnn_model_2.pt'\n",
    "}\n",
    "model7 = training_loop(config, rnn_model, train_set, dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [row['tokens'] for index,row in dftrain.iterrows()]\n",
    "dev_tokens = [row['tokens'] for index,row in dfdev.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2index = {'OAG':2,'CAG':0,'NAG':1}\n",
    "index2class = {2:'OAG',0:'CAG',1:'NAG'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model7(dev_tokens)\n",
    "dev_pred = out.cpu().argmax(1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_label=[]\n",
    "for i in range(len(dev_set)):\n",
    "    dev_label.append(dev_set[i]['labelnumber'])\n",
    "dev_pred_class = [index2class[x]  for x in dev_pred]\n",
    "dev_label_class = [index2class[x]  for x in dev_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAG', 'NAG', 'NAG', 'NAG', 'OAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'NAG', 'NAG', 'NAG', 'OAG', 'CAG', 'CAG', 'CAG', 'CAG', 'OAG', 'NAG']\n",
      "['NAG', 'NAG', 'OAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'OAG', 'NAG', 'CAG', 'NAG', 'NAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'NAG']\n"
     ]
    }
   ],
   "source": [
    "print(dev_label_class[:20])\n",
    "print(dev_pred_class[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         OAG       0.41      0.55      0.47       700\n",
      "         NAG       0.54      0.56      0.55       815\n",
      "         CAG       0.39      0.19      0.26       485\n",
      "\n",
      "    accuracy                           0.47      2000\n",
      "   macro avg       0.45      0.43      0.43      2000\n",
      "weighted avg       0.46      0.47      0.45      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['OAG', 'NAG', 'CAG']\n",
    "print(sm.classification_report(dev_label_class, dev_pred_class,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 93 249 143]\n",
      " [ 79 383 238]\n",
      " [ 65 294 456]]\n"
     ]
    }
   ],
   "source": [
    "print(sm.confusion_matrix(dev_label_class, dev_pred_class, labels=['OAG', 'CAG', 'NAG']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "skplt.metrics.plot_confusion_matrix(dev_label_class, dev_pred_class, normalize=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = pd.read_pickle('test_tokenized_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = dftest.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_, test_set_, batch_size_):\n",
    "    model = model_\n",
    "    model_.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(\"cpu\")\n",
    "\n",
    "    batch_tokens = []\n",
    "    predictions = []\n",
    "    batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_set_)):\n",
    "            \n",
    "            batch_tokens.append(test_set_[i]['tokens'])\n",
    "\n",
    "            if len(batch_tokens) == batch_size_ or i == (len(test_set_)-1):\n",
    "                batch_count+=1\n",
    "                out = model(batch_tokens)\n",
    "                #print(\"out\",out)\n",
    "                y_pred = out.cpu()\n",
    "                #print(\"y_pred\",y_pred)\n",
    "\n",
    "                predictions.extend(y_pred.argmax(1).tolist())\n",
    "                \n",
    "                #print(batch_count,len(batch_tokens))\n",
    "                batch_tokens = []\n",
    "                \n",
    "    \n",
    "    #print(\"evaluate actual\",actual)\n",
    "    #print(\"evaluate predic\",predictions)\n",
    "    #accuracy = accuracy_score(actual, predictions)\n",
    "    \n",
    "    #print(accurate,len(test_set_))\n",
    "    \n",
    "    #print(\"evaluate loss\",total_loss/len(test_set), \"accurary\",accurate/len(test_set))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test(model7,test_set, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedlabels = [index2class[x] for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1001, 1001)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictedlabels), len(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest['Label'] = predictedlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facebook_corpus_msr_495558</td>\n",
       "      <td>CAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facebook_corpus_msr_1561809</td>\n",
       "      <td>OAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook_corpus_msr_442487</td>\n",
       "      <td>CAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>facebook_corpus_msr_495517</td>\n",
       "      <td>NAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebook_corpus_msr_1805455</td>\n",
       "      <td>NAG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ID Label\n",
       "0   facebook_corpus_msr_495558   CAG\n",
       "1  facebook_corpus_msr_1561809   OAG\n",
       "2   facebook_corpus_msr_442487   CAG\n",
       "3   facebook_corpus_msr_495517   NAG\n",
       "4  facebook_corpus_msr_1805455   NAG"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest[['ID','Label']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest.to_csv('prediction_rnn_all.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.load('bigru_model.pt')\n",
    "bigru_model.load_state_dict(state['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'quality',\n",
       "  'of',\n",
       "  're',\n",
       "  'made',\n",
       "  'now',\n",
       "  'makes',\n",
       "  'me',\n",
       "  'think',\n",
       "  'it',\n",
       "  'is',\n",
       "  'something',\n",
       "  'to',\n",
       "  'be',\n",
       "  'bought',\n",
       "  'from',\n",
       "  'fish',\n",
       "  'market'],\n",
       " ['siva',\n",
       "  'how',\n",
       "  'is',\n",
       "  'ur',\n",
       "  'mother',\n",
       "  'how',\n",
       "  'is',\n",
       "  'ur',\n",
       "  'wife',\n",
       "  'how',\n",
       "  'is',\n",
       "  'ur',\n",
       "  'sister',\n",
       "  'hope',\n",
       "  'everyone',\n",
       "  'is',\n",
       "  'fine']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "out =  model7(dev_tokens)\n",
    "dev_pred = out.cpu().argmax(1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_pred[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred_class = [index2class[x]  for x in dev_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dev_pred_class).to_csv('dev_prediction_rnn.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
