{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JxrcJu64gGdz"
   },
   "outputs": [],
   "source": [
    "#uses pandas and then convert to list/dict\n",
    "import pandas as pd \n",
    "import string\n",
    "from string import punctuation\n",
    "import re\n",
    "import emoji\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.utils import shuffle\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import itertools\n",
    "import time\n",
    "import contractions\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn.metrics as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwBf8mpmhML7"
   },
   "outputs": [],
   "source": [
    "data_folder = 'data/'\n",
    "#glove_folder = 'E:/rajiur/workspace/nlp/data/'\n",
    "glove_folder = 'D:/workspace/nlp/glove/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = pd.read_pickle('train_tokenized_df.pkl')\n",
    "dfdev = pd.read_pickle('dev_tokenized_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Label</th>\n",
       "      <th>labelnumber</th>\n",
       "      <th>commentnotlowered</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokensnotlowered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facebook_corpus_msr_1723796</td>\n",
       "      <td>well said sonu  you have courage to stand agai...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>2</td>\n",
       "      <td>well said sonu  you have courage to stand agai...</td>\n",
       "      <td>[well, said, sonu, you, have, courage, to, sta...</td>\n",
       "      <td>[well, said, sonu, you, have, courage, to, sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facebook_corpus_msr_466073</td>\n",
       "      <td>most of private banks atm s like hdfc  icici e...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "      <td>most of private banks atm s like hdfc  icici e...</td>\n",
       "      <td>[most, of, private, banks, atm, s, like, hdfc,...</td>\n",
       "      <td>[most, of, private, banks, atm, s, like, hdfc,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook_corpus_msr_1493901</td>\n",
       "      <td>now question is  pakistan will adhere to this</td>\n",
       "      <td>OAG</td>\n",
       "      <td>2</td>\n",
       "      <td>now question is  pakistan will adhere to this</td>\n",
       "      <td>[now, question, is, pakistan, will, adhere, to...</td>\n",
       "      <td>[now, question, is, pakistan, will, adhere, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>facebook_corpus_msr_405512</td>\n",
       "      <td>pakistan is comprised of fake muslims who does...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>2</td>\n",
       "      <td>pakistan is comprised of fake muslims who does...</td>\n",
       "      <td>[pakistan, is, comprised, of, fake, muslims, w...</td>\n",
       "      <td>[pakistan, is, comprised, of, fake, muslims, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebook_corpus_msr_1521685</td>\n",
       "      <td>we r against cow slaughter so of course it w...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "      <td>we r against cow slaughter so of course it w...</td>\n",
       "      <td>[we, r, against, cow, slaughter, so, of, cours...</td>\n",
       "      <td>[we, r, against, cow, slaughter, so, of, cours...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ID  \\\n",
       "0  facebook_corpus_msr_1723796   \n",
       "1   facebook_corpus_msr_466073   \n",
       "2  facebook_corpus_msr_1493901   \n",
       "3   facebook_corpus_msr_405512   \n",
       "4  facebook_corpus_msr_1521685   \n",
       "\n",
       "                                             Comment Label  labelnumber  \\\n",
       "0  well said sonu  you have courage to stand agai...   OAG            2   \n",
       "1  most of private banks atm s like hdfc  icici e...   NAG            1   \n",
       "2     now question is  pakistan will adhere to this    OAG            2   \n",
       "3  pakistan is comprised of fake muslims who does...   OAG            2   \n",
       "4    we r against cow slaughter so of course it w...   NAG            1   \n",
       "\n",
       "                                   commentnotlowered  \\\n",
       "0  well said sonu  you have courage to stand agai...   \n",
       "1  most of private banks atm s like hdfc  icici e...   \n",
       "2     now question is  pakistan will adhere to this    \n",
       "3  pakistan is comprised of fake muslims who does...   \n",
       "4    we r against cow slaughter so of course it w...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [well, said, sonu, you, have, courage, to, sta...   \n",
       "1  [most, of, private, banks, atm, s, like, hdfc,...   \n",
       "2  [now, question, is, pakistan, will, adhere, to...   \n",
       "3  [pakistan, is, comprised, of, fake, muslims, w...   \n",
       "4  [we, r, against, cow, slaughter, so, of, cours...   \n",
       "\n",
       "                                    tokensnotlowered  \n",
       "0  [well, said, sonu, you, have, courage, to, sta...  \n",
       "1  [most, of, private, banks, atm, s, like, hdfc,...  \n",
       "2  [now, question, is, pakistan, will, adhere, to...  \n",
       "3  [pakistan, is, comprised, of, fake, muslims, w...  \n",
       "4  [we, r, against, cow, slaughter, so, of, cours...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set = dfdev.to_dict('records')\n",
    "train_set = dftrain.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.pkl', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    vocabulary = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22138"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Embedder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YodY1Tg6jPZP"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class WordEmbedder(nn.Module):\n",
    "    def __init__(self, vocab, glove_file, seqlen = 300, varlen = False):\n",
    "        super(WordEmbedder, self).__init__()\n",
    "        assert os.path.exists(glove_file) and glove_file.endswith('.txt'), glove_file\n",
    "        \n",
    "        self.emb_dim = None\n",
    "        \n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.UNK_TOKEN = '<UNK>'\n",
    "        self.sequence_length = seqlen\n",
    "        self.various_length = varlen\n",
    "        \n",
    "        idx2word = [self.PAD_TOKEN, self.UNK_TOKEN]\n",
    "        idx2vect = [None, None]\n",
    "        \n",
    "        with open(glove_file, 'r', encoding='utf-8') as fp:\n",
    "            for line in fp:\n",
    "                line = line.split()\n",
    "                \n",
    "                if line[0] not in vocab:\n",
    "                    continue\n",
    "                \n",
    "                w = line[0]\n",
    "                v = np.array([float(value) for value in line[1:]])\n",
    "                \n",
    "                if self.emb_dim is None:\n",
    "                    self.emb_dim = v.shape[0]\n",
    "            \n",
    "                idx2word.append(w)\n",
    "                idx2vect.append(v)\n",
    "                \n",
    "        idx2vect[0] = np.zeros(self.emb_dim)\n",
    "        idx2vect[1] = np.mean(idx2vect[2:], axis=0)\n",
    "    \n",
    "        self.embeddings = torch.from_numpy(np.array(idx2vect)).float()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(self.embeddings, freeze=False)\n",
    "        \n",
    "        self.idx2word = {i: w for i, w in enumerate(idx2word)}\n",
    "        self.word2idx = {w: i for i, w in self.idx2word.items()}\n",
    "    \n",
    "    def forward(self, samples):\n",
    "        pad_idx = self.word2idx[self.PAD_TOKEN]\n",
    "        unk_idx = self.word2idx[self.UNK_TOKEN]\n",
    "        \n",
    "        if self.various_length:\n",
    "\n",
    "            #Find the length of the longest sample\n",
    "            maxlen = max([len(s) for s in samples])\n",
    "        \n",
    "        else:\n",
    "\n",
    "            #Use a constant length for all samples\n",
    "            maxlen = self.sequence_length\n",
    "        \n",
    "        encoded = [[self.word2idx.get(token, unk_idx) for token in tokens] for tokens in samples]\n",
    "        \n",
    "        padded = np.zeros((len(samples), maxlen), dtype=int)\n",
    "        masks = torch.zeros(len(samples), maxlen).long()\n",
    "        \n",
    "        # Padding and masking\n",
    "        for i in range(len(encoded)):\n",
    "            masks[i, :len(encoded[i])] = 1\n",
    "            padded[i, :len(encoded[i])] = np.array(encoded[i])[:maxlen]\n",
    "            # encoded[i] += [pad_idx] * max(0, (maxlen - len(encoded[i])))\n",
    "        \n",
    "        encoded = torch.tensor(padded).long()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            encoded = encoded.cuda()\n",
    "            masks = masks.cuda()\n",
    "        \n",
    "        result = {\n",
    "            'output': self.embeddings(encoded),\n",
    "            'mask': masks,\n",
    "            'encoded': encoded\n",
    "        }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "pEp3vP0ljUk8",
    "outputId": "0910836d-5434-4610-8c5e-f526d6587664"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordEmbedder(\n",
       "  (embeddings): Embedding(19447, 300)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = torch.load('embedder_glove_300d.pt')\n",
    "embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.WordEmbedder"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_-JCBPkkoNq"
   },
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a2bFiexQks5Y"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def track_best_model(model_path, model, epoch, best_acc, dev_acc, dev_loss):\n",
    "    if best_acc > dev_acc:\n",
    "        return best_acc, ''\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'acc': dev_acc,\n",
    "        'loss': dev_loss,\n",
    "        'model': model.state_dict()\n",
    "    }\n",
    "    torch.save(state, model_path)\n",
    "    return dev_acc, ' * '\n",
    "\n",
    "def hot_target(batch_target):\n",
    "    result = []\n",
    "    for target in batch_target:\n",
    "        init_target = [0,0,0]\n",
    "        init_target[target]=1\n",
    "        result.append(init_target)\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XQxNjqZEktf_"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def train(model, optimizer, shuffled_train_set, batch_size, loss_func=\"CE\"):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss().to(\"cpu\")\n",
    "    total_loss = 0\n",
    "    batch_tokens, batch_target = [], []\n",
    "\n",
    "    random.Random(1234).shuffle(shuffled_train_set)\n",
    "\n",
    "    for i in range(len(shuffled_train_set)):\n",
    "\n",
    "        batch_tokens.append(shuffled_train_set[i]['tokens'])\n",
    "        batch_target.append(shuffled_train_set[i]['labelnumber'])\n",
    "\n",
    "        if len(batch_tokens) == batch_size or i == (len(shuffled_train_set) - 1):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(batch_tokens)\n",
    "            y_pred = out.cpu()\n",
    "\n",
    "            if(loss_func=='CE'):\n",
    "                #loss method 1: CrossEntropyLoss\n",
    "                loss = criterion(y_pred, torch.tensor(batch_target))\n",
    "            else:\n",
    "                loss = torch.nn.functional.binary_cross_entropy(y_pred,torch.tensor(hot_target(batch_target)).float())\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_tokens, batch_target = [], []\n",
    "        \n",
    "    return model, shuffled_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5sZMJ9fku9k"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dev_set, batch_size, loss_func='CE'):\n",
    "\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(\"cpu\")\n",
    "    total_loss = 0\n",
    "    accurate = 0\n",
    "    batch_tokens, batch_target = [], []\n",
    "    predictions, actual = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dev_set)):\n",
    "            \n",
    "            batch_tokens.append(dev_set[i]['tokens'])\n",
    "            batch_target.append(dev_set[i]['labelnumber'])\n",
    "\n",
    "            if len(batch_tokens) == batch_size or i == (len(dev_set)-1):\n",
    "\n",
    "                out = model(batch_tokens)\n",
    "                #print(\"out\",out)\n",
    "                y_pred = out.cpu()\n",
    "                \n",
    "                predictions.extend(y_pred.argmax(1).tolist())\n",
    "                actual.extend(batch_target)\n",
    "               \n",
    "                if(loss_func=='CE'):\n",
    "                    loss = criterion(y_pred, torch.tensor(batch_target))\n",
    "                else:\n",
    "                    loss = torch.nn.functional.binary_cross_entropy(y_pred,torch.tensor(hot_target(batch_target)).float())\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                accurate += (y_pred.argmax(1) == torch.tensor(batch_target)).sum().item()\n",
    "                \n",
    "                 \n",
    "\n",
    "                batch_tokens, batch_target = [], []\n",
    "\n",
    "    return predictions, total_loss/len(dev_set), accurate/len(dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ko3ufksekwnL"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def training_loop(config, model_, train_set, dev_set, loss_func='CE'):\n",
    "\n",
    "    model = model_\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=config['momentum'])\n",
    "\n",
    "    shuffled_train_set = train_set\n",
    "    best_acc = 0\n",
    "    \n",
    "\n",
    "    for epoch in range(config['epoch']):\n",
    "\n",
    "        epoch_msg = '[Epoch {}] / {}'.format(epoch+1, config['epoch'])\n",
    "\n",
    "        model, shuffled_train_set = train(model, optimizer, shuffled_train_set, batch_size=64, loss_func=loss_func)\n",
    "\n",
    "        train_pred, train_loss, train_acc = evaluate(model, shuffled_train_set, batch_size=128, loss_func=loss_func)\n",
    "        epoch_msg += ' [TRAIN] Loss: {:.4f}, Acc: {:.4f}'.format(train_loss, train_acc)\n",
    "        val_pred, val_loss, val_acc = evaluate(model, dev_set, batch_size=128, loss_func=loss_func)\n",
    "        epoch_msg += ' [DEV] Loss: {:.4f}, Acc: {:.4f}'.format(val_loss, val_acc)\n",
    "        #test_pred, test_loss, test_acc = evaluate(model, test_set, batch_size=128)\n",
    "        #epoch_msg += ' [TEST] Loss: {:.4f}, Acc: {:.4f}'.format(test_loss, test_acc)\n",
    "\n",
    "        best_acc, epoch_track = track_best_model(config['checkpoint'], model, epoch, best_acc, val_acc, val_loss)\n",
    "        print(epoch_msg + epoch_track)\n",
    "\n",
    "    print('Done Training!')\n",
    "\n",
    "    state = torch.load(config['checkpoint'])\n",
    "    model.load_state_dict(state['model'])\n",
    "\n",
    "    print('Returning best model from epoch {} with loss {:.5f} and accuracy {:.5f}'.format(state['epoch'], state['loss'], state['acc']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pb59sWVskYxT"
   },
   "source": [
    "**MulBiGRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulBiGRULayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=2, drop=0.3):\n",
    "        super(MulBiGRULayer, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim // 2, bidirectional=True, num_layers=num_layers, dropout=drop)\n",
    "\n",
    "    def forward(self, vectors, mask):\n",
    "        batch_size = vectors.size(0)\n",
    "        maxlen = vectors.size(1)\n",
    "        lengths = mask.sum(-1)\n",
    "        \n",
    "        gru_out, _ = self.gru(vectors)  # (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        assert gru_out.size(0) == batch_size\n",
    "        assert gru_out.size(1) == maxlen\n",
    "        assert gru_out.size(2) == self.hidden_dim\n",
    "\n",
    "        # Separate the directions of the GRU\n",
    "        gru_out = gru_out.view(batch_size, maxlen, 2, self.hidden_dim // 2)\n",
    "\n",
    "        # Pick up the last hidden state per direction\n",
    "        fw_last_hn = gru_out[range(batch_size), lengths - 1, 0]  # (batch, hidden // 2)\n",
    "        bw_last_hn = gru_out[range(batch_size), 0, 1]            # (batch, hidden // 2)\n",
    "        \n",
    "        last_hn = torch.sigmoid(torch.cat([fw_last_hn, bw_last_hn], dim=1))     # (batch, hidden // 2) -> (batch, hidden)\n",
    "\n",
    "        return {'output': last_hn, 'outputs': gru_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vWAgU01GkXmC"
   },
   "outputs": [],
   "source": [
    "class MyClassifier(nn.Module):\n",
    "    def __init__(self, embedder, extractor):\n",
    "        super(MyClassifier, self).__init__()\n",
    "        self.embedder = embedder\n",
    "        self.extractor = extractor\n",
    "        self.classifier = nn.Linear(extractor.hidden_dim, 3)\n",
    "        #self.fc1 = nn.Linear(extractor.hidden_dim, 128)\n",
    "        #self.fc2 = nn.Linear(128, 64)\n",
    "        #self.classifier = nn.Linear(64, 3)\n",
    "        \n",
    "    def forward(self, tokens, targets=None):\n",
    "        embedded = self.embedder(tokens)\n",
    "        extracted = self.extractor(embedded['output'], embedded['mask'])\n",
    "        \n",
    "        #print(type(self.classifier(extracted['output'])),self.classifier(extracted['output']).shape,self.classifier(extracted['output']))\n",
    "        #print(type(torch.sigmoid(self.classifier(extracted['output']))), torch.sigmoid(self.classifier(extracted['output'])).shape, torch.sigmoid(self.classifier(extracted['output'])))\n",
    "        #print(torch.softmax(self.classifier(extracted['output']), dim=1))\n",
    "        \n",
    "        #x = self.fc1(extracted['output'])\n",
    "        #x = self.fc2(x)\n",
    "        #output = torch.softmax(self.classifier(x),dim=1)\n",
    "        output = torch.softmax(self.classifier(extracted['output']), dim=1)\n",
    "       \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YodY1Tg6jPZP"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class WordEmbedder(nn.Module):\n",
    "    def __init__(self, vocab, glove_file, seqlen = 300, varlen = False):\n",
    "        super(WordEmbedder, self).__init__()\n",
    "        assert os.path.exists(glove_file) and glove_file.endswith('.txt'), glove_file\n",
    "        \n",
    "        self.emb_dim = None\n",
    "        \n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.UNK_TOKEN = '<UNK>'\n",
    "        self.sequence_length = seqlen\n",
    "        self.various_length = varlen\n",
    "        \n",
    "        idx2word = [self.PAD_TOKEN, self.UNK_TOKEN]\n",
    "        idx2vect = [None, None]\n",
    "        \n",
    "        with open(glove_file, 'r', encoding='utf-8') as fp:\n",
    "            for line in fp:\n",
    "                line = line.split()\n",
    "                \n",
    "                if line[0] not in vocab:\n",
    "                    continue\n",
    "                \n",
    "                w = line[0]\n",
    "                v = np.array([float(value) for value in line[1:]])\n",
    "                \n",
    "                if self.emb_dim is None:\n",
    "                    self.emb_dim = v.shape[0]\n",
    "            \n",
    "                idx2word.append(w)\n",
    "                idx2vect.append(v)\n",
    "                \n",
    "        idx2vect[0] = np.zeros(self.emb_dim)\n",
    "        idx2vect[1] = np.mean(idx2vect[2:], axis=0)\n",
    "    \n",
    "        self.embeddings = torch.from_numpy(np.array(idx2vect)).float()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(self.embeddings, freeze=False)\n",
    "        \n",
    "        self.idx2word = {i: w for i, w in enumerate(idx2word)}\n",
    "        self.word2idx = {w: i for i, w in self.idx2word.items()}\n",
    "    \n",
    "    def forward(self, samples):\n",
    "        pad_idx = self.word2idx[self.PAD_TOKEN]\n",
    "        unk_idx = self.word2idx[self.UNK_TOKEN]\n",
    "        \n",
    "        if self.various_length:\n",
    "\n",
    "            #Find the length of the longest sample\n",
    "            maxlen = max([len(s) for s in samples])\n",
    "        \n",
    "        else:\n",
    "\n",
    "            #Use a constant length for all samples\n",
    "            maxlen = self.sequence_length\n",
    "        \n",
    "        encoded = [[self.word2idx.get(token, unk_idx) for token in tokens] for tokens in samples]\n",
    "        \n",
    "        padded = np.zeros((len(samples), maxlen), dtype=int)\n",
    "        masks = torch.zeros(len(samples), maxlen).long()\n",
    "        \n",
    "        # Padding and masking\n",
    "        for i in range(len(encoded)):\n",
    "            masks[i, :len(encoded[i])] = 1\n",
    "            padded[i, :len(encoded[i])] = np.array(encoded[i])[:maxlen]\n",
    "            # encoded[i] += [pad_idx] * max(0, (maxlen - len(encoded[i])))\n",
    "        \n",
    "        encoded = torch.tensor(padded).long()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            encoded = encoded.cuda()\n",
    "            masks = masks.cuda()\n",
    "        \n",
    "        result = {\n",
    "            'output': self.embeddings(encoded),\n",
    "            'mask': masks,\n",
    "            'encoded': encoded\n",
    "        }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MulBiGRULayer(\n",
       "  (gru): GRU(300, 64, num_layers=2, dropout=0.3, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mulbigru_layer = MulBiGRULayer(embedder.emb_dim, 128)\n",
    "mulbigru_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyClassifier(\n",
       "  (embedder): WordEmbedder(\n",
       "    (embeddings): Embedding(19447, 300)\n",
       "  )\n",
       "  (extractor): MulBiGRULayer(\n",
       "    (gru): GRU(300, 64, num_layers=2, dropout=0.3, bidirectional=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mulbigru_model = MyClassifier(embedder, mulbigru_layer)\n",
    "mulbigru_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] / 100 [TRAIN] Loss: 0.0084, Acc: 0.4243 [DEV] Loss: 0.0087, Acc: 0.4145 * \n",
      "[Epoch 2] / 100 [TRAIN] Loss: 0.0082, Acc: 0.4495 [DEV] Loss: 0.0085, Acc: 0.4320 * \n",
      "[Epoch 3] / 100 [TRAIN] Loss: 0.0083, Acc: 0.4577 [DEV] Loss: 0.0086, Acc: 0.4415 * \n",
      "[Epoch 4] / 100 [TRAIN] Loss: 0.0080, Acc: 0.4978 [DEV] Loss: 0.0084, Acc: 0.4740 * \n",
      "[Epoch 5] / 100 [TRAIN] Loss: 0.0079, Acc: 0.5211 [DEV] Loss: 0.0083, Acc: 0.4705\n",
      "[Epoch 6] / 100 [TRAIN] Loss: 0.0078, Acc: 0.5294 [DEV] Loss: 0.0083, Acc: 0.4720\n",
      "[Epoch 7] / 100 [TRAIN] Loss: 0.0078, Acc: 0.5285 [DEV] Loss: 0.0083, Acc: 0.4650\n",
      "[Epoch 8] / 100 [TRAIN] Loss: 0.0077, Acc: 0.5250 [DEV] Loss: 0.0083, Acc: 0.4645\n",
      "[Epoch 9] / 100 [TRAIN] Loss: 0.0076, Acc: 0.5488 [DEV] Loss: 0.0083, Acc: 0.4685\n",
      "[Epoch 10] / 100 [TRAIN] Loss: 0.0078, Acc: 0.5135 [DEV] Loss: 0.0083, Acc: 0.4570\n",
      "[Epoch 11] / 100 [TRAIN] Loss: 0.0075, Acc: 0.5772 [DEV] Loss: 0.0083, Acc: 0.4740 * \n",
      "[Epoch 12] / 100 [TRAIN] Loss: 0.0075, Acc: 0.5782 [DEV] Loss: 0.0083, Acc: 0.4730\n",
      "[Epoch 13] / 100 [TRAIN] Loss: 0.0073, Acc: 0.6128 [DEV] Loss: 0.0083, Acc: 0.4805 * \n",
      "[Epoch 14] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6289 [DEV] Loss: 0.0083, Acc: 0.4815 * \n",
      "[Epoch 15] / 100 [TRAIN] Loss: 0.0072, Acc: 0.6187 [DEV] Loss: 0.0084, Acc: 0.4740\n",
      "[Epoch 16] / 100 [TRAIN] Loss: 0.0071, Acc: 0.6408 [DEV] Loss: 0.0083, Acc: 0.4755\n",
      "[Epoch 17] / 100 [TRAIN] Loss: 0.0070, Acc: 0.6610 [DEV] Loss: 0.0083, Acc: 0.4875 * \n",
      "[Epoch 18] / 100 [TRAIN] Loss: 0.0069, Acc: 0.6638 [DEV] Loss: 0.0083, Acc: 0.4860\n",
      "[Epoch 19] / 100 [TRAIN] Loss: 0.0068, Acc: 0.6761 [DEV] Loss: 0.0083, Acc: 0.4850\n",
      "[Epoch 20] / 100 [TRAIN] Loss: 0.0069, Acc: 0.6684 [DEV] Loss: 0.0084, Acc: 0.4865\n",
      "[Epoch 21] / 100 [TRAIN] Loss: 0.0068, Acc: 0.6738 [DEV] Loss: 0.0084, Acc: 0.4725\n",
      "[Epoch 22] / 100 [TRAIN] Loss: 0.0068, Acc: 0.6797 [DEV] Loss: 0.0085, Acc: 0.4600\n",
      "[Epoch 23] / 100 [TRAIN] Loss: 0.0067, Acc: 0.6997 [DEV] Loss: 0.0084, Acc: 0.4755\n",
      "[Epoch 24] / 100 [TRAIN] Loss: 0.0066, Acc: 0.7017 [DEV] Loss: 0.0084, Acc: 0.4785\n",
      "[Epoch 25] / 100 [TRAIN] Loss: 0.0066, Acc: 0.7068 [DEV] Loss: 0.0084, Acc: 0.4775\n",
      "[Epoch 26] / 100 [TRAIN] Loss: 0.0066, Acc: 0.7116 [DEV] Loss: 0.0084, Acc: 0.4690\n",
      "[Epoch 27] / 100 [TRAIN] Loss: 0.0066, Acc: 0.7127 [DEV] Loss: 0.0085, Acc: 0.4695\n",
      "[Epoch 28] / 100 [TRAIN] Loss: 0.0065, Acc: 0.7147 [DEV] Loss: 0.0084, Acc: 0.4800\n",
      "[Epoch 29] / 100 [TRAIN] Loss: 0.0066, Acc: 0.7088 [DEV] Loss: 0.0084, Acc: 0.4815\n",
      "[Epoch 30] / 100 [TRAIN] Loss: 0.0065, Acc: 0.7242 [DEV] Loss: 0.0084, Acc: 0.4815\n",
      "[Epoch 31] / 100 [TRAIN] Loss: 0.0065, Acc: 0.7226 [DEV] Loss: 0.0084, Acc: 0.4725\n",
      "[Epoch 32] / 100 [TRAIN] Loss: 0.0065, Acc: 0.7199 [DEV] Loss: 0.0085, Acc: 0.4665\n",
      "[Epoch 33] / 100 [TRAIN] Loss: 0.0064, Acc: 0.7279 [DEV] Loss: 0.0084, Acc: 0.4710\n",
      "[Epoch 34] / 100 [TRAIN] Loss: 0.0064, Acc: 0.7319 [DEV] Loss: 0.0085, Acc: 0.4685\n",
      "[Epoch 35] / 100 [TRAIN] Loss: 0.0064, Acc: 0.7336 [DEV] Loss: 0.0084, Acc: 0.4770\n",
      "[Epoch 36] / 100 [TRAIN] Loss: 0.0064, Acc: 0.7324 [DEV] Loss: 0.0085, Acc: 0.4745\n",
      "[Epoch 37] / 100 [TRAIN] Loss: 0.0064, Acc: 0.7396 [DEV] Loss: 0.0085, Acc: 0.4740\n",
      "[Epoch 38] / 100 [TRAIN] Loss: 0.0064, Acc: 0.7362 [DEV] Loss: 0.0085, Acc: 0.4715\n",
      "[Epoch 39] / 100 [TRAIN] Loss: 0.0064, Acc: 0.7355 [DEV] Loss: 0.0085, Acc: 0.4715\n",
      "[Epoch 40] / 100 [TRAIN] Loss: 0.0064, Acc: 0.7385 [DEV] Loss: 0.0085, Acc: 0.4735\n",
      "[Epoch 41] / 100 [TRAIN] Loss: 0.0063, Acc: 0.7416 [DEV] Loss: 0.0085, Acc: 0.4730\n",
      "[Epoch 42] / 100 [TRAIN] Loss: 0.0063, Acc: 0.7418 [DEV] Loss: 0.0085, Acc: 0.4745\n",
      "[Epoch 43] / 100 [TRAIN] Loss: 0.0063, Acc: 0.7466 [DEV] Loss: 0.0085, Acc: 0.4725\n",
      "[Epoch 44] / 100 [TRAIN] Loss: 0.0063, Acc: 0.7450 [DEV] Loss: 0.0085, Acc: 0.4670\n",
      "[Epoch 45] / 100 [TRAIN] Loss: 0.0063, Acc: 0.7458 [DEV] Loss: 0.0085, Acc: 0.4655\n",
      "[Epoch 46] / 100 [TRAIN] Loss: 0.0063, Acc: 0.7492 [DEV] Loss: 0.0085, Acc: 0.4615\n",
      "[Epoch 47] / 100 [TRAIN] Loss: 0.0063, Acc: 0.7452 [DEV] Loss: 0.0085, Acc: 0.4710\n",
      "[Epoch 48] / 100 [TRAIN] Loss: 0.0063, Acc: 0.7520 [DEV] Loss: 0.0085, Acc: 0.4675\n",
      "[Epoch 49] / 100 [TRAIN] Loss: 0.0062, Acc: 0.7558 [DEV] Loss: 0.0085, Acc: 0.4635\n",
      "[Epoch 50] / 100 [TRAIN] Loss: 0.0062, Acc: 0.7541 [DEV] Loss: 0.0086, Acc: 0.4625\n",
      "[Epoch 51] / 100 [TRAIN] Loss: 0.0063, Acc: 0.7528 [DEV] Loss: 0.0085, Acc: 0.4605\n",
      "[Epoch 52] / 100 [TRAIN] Loss: 0.0063, Acc: 0.7516 [DEV] Loss: 0.0085, Acc: 0.4685\n",
      "[Epoch 53] / 100 [TRAIN] Loss: 0.0062, Acc: 0.7568 [DEV] Loss: 0.0086, Acc: 0.4665\n",
      "[Epoch 54] / 100 [TRAIN] Loss: 0.0062, Acc: 0.7573 [DEV] Loss: 0.0086, Acc: 0.4660\n",
      "[Epoch 55] / 100 [TRAIN] Loss: 0.0062, Acc: 0.7582 [DEV] Loss: 0.0085, Acc: 0.4665\n",
      "[Epoch 56] / 100 [TRAIN] Loss: 0.0062, Acc: 0.7557 [DEV] Loss: 0.0085, Acc: 0.4660\n",
      "[Epoch 57] / 100 [TRAIN] Loss: 0.0062, Acc: 0.7612 [DEV] Loss: 0.0086, Acc: 0.4675\n",
      "[Epoch 58] / 100 [TRAIN] Loss: 0.0062, Acc: 0.7604 [DEV] Loss: 0.0085, Acc: 0.4745\n",
      "[Epoch 59] / 100 [TRAIN] Loss: 0.0062, Acc: 0.7628 [DEV] Loss: 0.0085, Acc: 0.4720\n",
      "[Epoch 60] / 100 [TRAIN] Loss: 0.0062, Acc: 0.7621 [DEV] Loss: 0.0085, Acc: 0.4705\n",
      "[Epoch 61] / 100 [TRAIN] Loss: 0.0062, Acc: 0.7602 [DEV] Loss: 0.0085, Acc: 0.4695\n",
      "[Epoch 62] / 100 [TRAIN] Loss: 0.0062, Acc: 0.7642 [DEV] Loss: 0.0085, Acc: 0.4700\n",
      "[Epoch 63] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7678 [DEV] Loss: 0.0086, Acc: 0.4640\n",
      "[Epoch 64] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7665 [DEV] Loss: 0.0085, Acc: 0.4710\n",
      "[Epoch 65] / 100 [TRAIN] Loss: 0.0062, Acc: 0.7655 [DEV] Loss: 0.0085, Acc: 0.4695\n",
      "[Epoch 66] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7691 [DEV] Loss: 0.0085, Acc: 0.4710\n",
      "[Epoch 67] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7700 [DEV] Loss: 0.0086, Acc: 0.4680\n",
      "[Epoch 68] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7699 [DEV] Loss: 0.0085, Acc: 0.4725\n",
      "[Epoch 69] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7702 [DEV] Loss: 0.0085, Acc: 0.4705\n",
      "[Epoch 70] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7715 [DEV] Loss: 0.0085, Acc: 0.4700\n",
      "[Epoch 71] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7718 [DEV] Loss: 0.0085, Acc: 0.4725\n",
      "[Epoch 72] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7719 [DEV] Loss: 0.0085, Acc: 0.4740\n",
      "[Epoch 73] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7698 [DEV] Loss: 0.0086, Acc: 0.4655\n",
      "[Epoch 74] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7729 [DEV] Loss: 0.0085, Acc: 0.4705\n",
      "[Epoch 75] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7726 [DEV] Loss: 0.0085, Acc: 0.4705\n",
      "[Epoch 76] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7735 [DEV] Loss: 0.0085, Acc: 0.4690\n",
      "[Epoch 77] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7732 [DEV] Loss: 0.0085, Acc: 0.4690\n",
      "[Epoch 78] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7715 [DEV] Loss: 0.0086, Acc: 0.4675\n",
      "[Epoch 79] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7742 [DEV] Loss: 0.0086, Acc: 0.4685\n",
      "[Epoch 80] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7743 [DEV] Loss: 0.0086, Acc: 0.4660\n",
      "[Epoch 81] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7732 [DEV] Loss: 0.0085, Acc: 0.4695\n",
      "[Epoch 82] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7752 [DEV] Loss: 0.0085, Acc: 0.4715\n",
      "[Epoch 83] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7764 [DEV] Loss: 0.0086, Acc: 0.4675\n",
      "[Epoch 84] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7751 [DEV] Loss: 0.0085, Acc: 0.4715\n",
      "[Epoch 85] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7772 [DEV] Loss: 0.0085, Acc: 0.4725\n",
      "[Epoch 86] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7788 [DEV] Loss: 0.0085, Acc: 0.4710\n",
      "[Epoch 87] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7767 [DEV] Loss: 0.0085, Acc: 0.4770\n",
      "[Epoch 88] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7774 [DEV] Loss: 0.0085, Acc: 0.4735\n",
      "[Epoch 89] / 100 [TRAIN] Loss: 0.0060, Acc: 0.7789 [DEV] Loss: 0.0085, Acc: 0.4740\n",
      "[Epoch 90] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7773 [DEV] Loss: 0.0085, Acc: 0.4725\n",
      "[Epoch 91] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7761 [DEV] Loss: 0.0085, Acc: 0.4740\n",
      "[Epoch 92] / 100 [TRAIN] Loss: 0.0061, Acc: 0.7765 [DEV] Loss: 0.0085, Acc: 0.4745\n",
      "[Epoch 93] / 100 [TRAIN] Loss: 0.0060, Acc: 0.7808 [DEV] Loss: 0.0085, Acc: 0.4705\n",
      "[Epoch 94] / 100 [TRAIN] Loss: 0.0060, Acc: 0.7815 [DEV] Loss: 0.0085, Acc: 0.4700\n",
      "[Epoch 95] / 100 [TRAIN] Loss: 0.0060, Acc: 0.7815 [DEV] Loss: 0.0085, Acc: 0.4695\n",
      "[Epoch 96] / 100 [TRAIN] Loss: 0.0060, Acc: 0.7823 [DEV] Loss: 0.0085, Acc: 0.4710\n",
      "[Epoch 97] / 100 [TRAIN] Loss: 0.0060, Acc: 0.7823 [DEV] Loss: 0.0085, Acc: 0.4690\n",
      "[Epoch 98] / 100 [TRAIN] Loss: 0.0060, Acc: 0.7808 [DEV] Loss: 0.0085, Acc: 0.4765\n",
      "[Epoch 99] / 100 [TRAIN] Loss: 0.0060, Acc: 0.7817 [DEV] Loss: 0.0085, Acc: 0.4760\n",
      "[Epoch 100] / 100 [TRAIN] Loss: 0.0060, Acc: 0.7804 [DEV] Loss: 0.0085, Acc: 0.4730\n",
      "Done Training!\n",
      "Returning best model from epoch 16 with loss 0.00831 and accuracy 0.48750\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'lr': 0.2,\n",
    "    'momentum': 0.90,\n",
    "    'epoch': 100,\n",
    "    'checkpoint': 'mulbigru_model.pt'\n",
    "}\n",
    "model4 = training_loop(config, mulbigru_model, train_set, dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [row['tokens'] for index,row in dftrain.iterrows()]\n",
    "dev_tokens = [row['tokens'] for index,row in dfdev.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2index = {'OAG':2,'CAG':0,'NAG':1}\n",
    "index2class = {2:'OAG',0:'CAG',1:'NAG'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model4(dev_tokens)\n",
    "dev_pred = out.cpu().argmax(1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_label=[]\n",
    "for i in range(len(dev_set)):\n",
    "    dev_label.append(dev_set[i]['labelnumber'])\n",
    "\n",
    "dev_label_class = [index2class[x]  for x in dev_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred_class = [index2class[x]  for x in dev_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAG', 'NAG', 'NAG', 'NAG', 'OAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'NAG', 'NAG', 'NAG', 'OAG', 'CAG', 'CAG', 'CAG', 'CAG', 'OAG', 'NAG']\n",
      "['NAG', 'CAG', 'OAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'OAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG']\n"
     ]
    }
   ],
   "source": [
    "print(dev_label_class[:20])\n",
    "print(dev_pred_class[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         OAG       0.42      0.54      0.47       700\n",
      "         NAG       0.56      0.58      0.57       815\n",
      "         CAG       0.49      0.26      0.34       485\n",
      "\n",
      "    accuracy                           0.49      2000\n",
      "   macro avg       0.49      0.46      0.46      2000\n",
      "weighted avg       0.49      0.49      0.48      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['OAG', 'NAG', 'CAG']\n",
    "print(sm.classification_report(dev_label_class, dev_pred_class,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[126 234 125]\n",
      " [ 82 379 239]\n",
      " [ 48 298 469]]\n"
     ]
    }
   ],
   "source": [
    "print(sm.confusion_matrix(dev_label_class, dev_pred_class, labels=['OAG', 'CAG', 'NAG']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev_label_class' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-87443f207409>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mskplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_label_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_pred_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dev_label_class' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "skplt.metrics.plot_confusion_matrix(dev_label_class, dev_pred_class, normalize=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = pd.read_pickle('test_tokenized_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = dftest.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_, test_set_, batch_size_):\n",
    "    model = model_\n",
    "    model_.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(\"cpu\")\n",
    "\n",
    "    batch_tokens = []\n",
    "    predictions = []\n",
    "    batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_set_)):\n",
    "            \n",
    "            batch_tokens.append(test_set_[i]['tokens'])\n",
    "\n",
    "            if len(batch_tokens) == batch_size_ or i == (len(test_set_)-1):\n",
    "                batch_count+=1\n",
    "                out = model(batch_tokens)\n",
    "                #print(\"out\",out)\n",
    "                y_pred = out.cpu()\n",
    "                #print(\"y_pred\",y_pred)\n",
    "\n",
    "                predictions.extend(y_pred.argmax(1).tolist())\n",
    "                \n",
    "                #print(batch_count,len(batch_tokens))\n",
    "                batch_tokens = []\n",
    "                \n",
    "    \n",
    "    #print(\"evaluate actual\",actual)\n",
    "    #print(\"evaluate predic\",predictions)\n",
    "    #accuracy = accuracy_score(actual, predictions)\n",
    "    \n",
    "    #print(accurate,len(test_set_))\n",
    "    \n",
    "    #print(\"evaluate loss\",total_loss/len(test_set), \"accurary\",accurate/len(test_set))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test(model4,test_set, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedlabels = [index2class[x] for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1001, 1001)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictedlabels), len(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest['Label'] = predictedlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facebook_corpus_msr_495558</td>\n",
       "      <td>CAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facebook_corpus_msr_1561809</td>\n",
       "      <td>CAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook_corpus_msr_442487</td>\n",
       "      <td>CAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>facebook_corpus_msr_495517</td>\n",
       "      <td>NAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebook_corpus_msr_1805455</td>\n",
       "      <td>CAG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ID Label\n",
       "0   facebook_corpus_msr_495558   CAG\n",
       "1  facebook_corpus_msr_1561809   CAG\n",
       "2   facebook_corpus_msr_442487   CAG\n",
       "3   facebook_corpus_msr_495517   NAG\n",
       "4  facebook_corpus_msr_1805455   CAG"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest[['ID','Label']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dftest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-dae2b3eedd81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdftest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'prediction_mulbigru_all.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dftest' is not defined"
     ]
    }
   ],
   "source": [
    "dftest.to_csv('prediction_mulbigru_all.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.load('mulbigru_model.pt')\n",
    "mulbigru_model.load_state_dict(state['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out =  mulbigru_model(dev_tokens)\n",
    "dev_pred = out.cpu().argmax(1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_pred[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred_class = [index2class[x]  for x in dev_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_label_class = [index2class[x]  for x in dev_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "skplt.metrics.plot_confusion_matrix(dev_label_class, dev_pred_class, normalize=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dev_pred_class).to_csv('dev_prediction_mulbigru.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
